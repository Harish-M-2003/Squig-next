"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[976],{1293:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var r=s(1527),i=s(8672);const o={sidebar_label:"Lexer",sidebar_position:3},t="Lexer",l={id:"developer-docs/lexer",title:"Lexer",description:"The Lexer class in Squig is responsible for performing lexical analysis, which involves breaking down the source code into tokens. Tokens are the smallest units of syntax in a Squig program and serve as the building blocks for further processing during compilation.",source:"@site/docs/developer-docs/lexer.md",sourceDirName:"developer-docs",slug:"/developer-docs/lexer",permalink:"/developer-docs/lexer",draft:!1,unlisted:!1,editUrl:"https://github.com/Harish-M-2003/Squig-Docusaurus/tree/main/docs/developer-docs/lexer.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_label:"Lexer",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Token",permalink:"/developer-docs/token"},next:{title:"Parser",permalink:"/developer-docs/parser"}},c={},d=[{value:"Lexer Class",id:"lexer-class",level:2},{value:"Methods:",id:"methods",level:3},{value:"Code Explanation:",id:"code-explanation",level:3},{value:"Example Usage:",id:"example-usage",level:3},{value:"Conclusion",id:"conclusion",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"lexer",children:"Lexer"}),"\n",(0,r.jsx)(n.p,{children:"The Lexer class in Squig is responsible for performing lexical analysis, which involves breaking down the source code into tokens. Tokens are the smallest units of syntax in a Squig program and serve as the building blocks for further processing during compilation."}),"\n",(0,r.jsx)(n.h2,{id:"lexer-class",children:"Lexer Class"}),"\n",(0,r.jsx)(n.p,{children:"The Lexer class processes the source code character by character, identifying and classifying tokens based on predefined rules. It utilizes various tokenization methods to recognize different types of tokens, including keywords, operators, literals, and special symbols."}),"\n",(0,r.jsx)(n.h3,{id:"methods",children:"Methods:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"__init__(self, file, source_code)"})}),": Initializes a new Lexer object with the specified source code and file name."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"next(self)"})}),": Moves to the next character in the source code."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenize_lesser_or_lesserThanEqual(self)"})}),": Tokenizes the lesser-than (",(0,r.jsx)(n.code,{children:"<"}),") or lesser-than-equal (",(0,r.jsx)(n.code,{children:"<="}),") operator."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenize_greater_or_greaterThanEqual(self)"})}),": Tokenizes the greater-than (",(0,r.jsx)(n.code,{children:">"}),") or greater-than-equal (",(0,r.jsx)(n.code,{children:">="}),") operator."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenize_not_or_notEqual(self)"})}),": Tokenizes the negation (",(0,r.jsx)(n.code,{children:"!"}),") or not-equal (",(0,r.jsx)(n.code,{children:"!="}),") operator."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenize_mul_or_power(self)"})}),": Tokenizes the multiplication (",(0,r.jsx)(n.code,{children:"*"}),") or power (",(0,r.jsx)(n.code,{children:"**"}),") operator."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenize_assignment(self)"})}),": Tokenizes assignment operators (",(0,r.jsx)(n.code,{children:":"}),", ",(0,r.jsx)(n.code,{children:":+"}),", ",(0,r.jsx)(n.code,{children:":*"}),", ",(0,r.jsx)(n.code,{children:":**"}),")."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenize_string(self)"})}),": Tokenizes string literals."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenize_variable(self)"})}),": Tokenizes variables."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenize_digit(self)"})}),": Tokenizes numeric literals."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenize_input_message(self)"})}),": Tokenizes input message strings."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"tokenize(self)"})}),": Performs tokenization of the source code, generating a list of tokens."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"code-explanation",children:"Code Explanation:"}),"\n",(0,r.jsx)(n.p,{children:"The provided code defines the Lexer class, which tokenizes the source code using various tokenization methods. Here's a breakdown of the code:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Initialization"}),": Initializes the Lexer object with the source code and file name."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"next()"})," Method"]}),": Advances to the next character in the source code."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tokenization Methods"}),": Define methods to tokenize different types of tokens, such as operators, literals, and special symbols."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"tokenize()"})," Method"]}),": Orchestrates the tokenization process by iterating through the source code and applying tokenization methods as needed."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-usage",children:"Example Usage:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from Lexer import Lexer\r\n\r\n# Initialize Lexer with source code\r\nlexer = Lexer("<core>", "if x < 10 { log \'x is less than 10\'; }")\r\n\r\n# Tokenize the source code\r\ntokens, error = lexer.tokenize()\r\n\r\nif error:\r\n    print("Error:", error)\r\nelse:\r\n    # Print tokens\r\n    for token in tokens:\r\n        print(token)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"The Lexer class in Squig plays a crucial role in the compilation process by breaking down source code into tokens. By accurately identifying and classifying tokens, Squig can perform subsequent parsing and interpretation tasks, leading to the execution of Squig programs."})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8672:(e,n,s)=>{s.d(n,{Z:()=>l,a:()=>t});var r=s(959);const i={},o=r.createContext(i);function t(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);